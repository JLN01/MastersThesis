{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from langchain_community.chat_models import AzureChatOpenAI\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using dotenv in order to access my secret keys, could be to the LLM, vector database, search etc. It keeps it connected to Azure\n",
    "dotenv_path = '/.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838def01a36d45488d447690a3848249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text_content = [page.extract_text() for page in pdf.pages if page.extract_text()]\n",
    "    return \"\\n\".join(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your PDF file\n",
    "pdf_path = \"ESG Knowledge Library\\Annual-Report-2023.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Creating a DataFrame including the source\n",
    "data = {\n",
    "    'text': [pdf_text],  # Assuming pdf_text contains the full text from your PDF\n",
    "    'source': [pdf_path]  # Include the PDF file name as source\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Dataset\n",
    "ds = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f08347edf1c4a6b938e9491961842c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "# Example operations on the dataset\n",
    "ds = ds.map(lambda example: {'length': len(example['text'])})  # Adding a column with the length of each text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bcbf13d66f4fc7b35ea917a4a8f5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "langchain_docs = [LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test context for the `@mui/material` library.\\n\\n## Installation\\n\\n```sh\\nnpm install @mui/material\\n```\\n\\n## Usage\\n\\n```jsx\\nimport React from \\'react\\';\\nimport { Button } from \\'@mui/material\\';\\n\\nfunction App() {\\n  return (\\n    <div className=\"App\">\\n      <Button variant=\"contained\" color=\"primary\">\\n        Hello World\\n      </Button>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Documentation\\n\\n- [Material-UI](https://material-ui.com/)\\n- [Material Design](https://material.io/)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from langchain.llms.base import LLM\n",
    "from typing import List\n",
    "import json\n",
    "from langchain.llms.base import BaseLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.retrievers import AzureAISearchRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hugging Face Inference Client\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llm_client = InferenceClient(model=repo_id, timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceInferenceLLM(LLM):\n",
    "    def __init__(self, model_id: str, timeout: int = 120):\n",
    "        self.inference_client = InferenceClient(model=model_id, timeout=timeout)\n",
    "\n",
    "    def _call(self, prompt: str, stop: List[str] = None) -> str:\n",
    "        response = self.inference_client.post(\n",
    "            json={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\"max_new_tokens\": 1000},\n",
    "                \"task\": \"text-generation\",\n",
    "            },\n",
    "        )\n",
    "        return json.loads(response.content.decode())[0][\"generated_text\"]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"huggingface-inference\"\n",
    "\n",
    "    def _generate(self, prompts: List[str], stop: List[str] = None) -> List[str]:\n",
    "        responses = []\n",
    "        for prompt in prompts:\n",
    "            response = self._call(prompt, stop)\n",
    "            responses.append(response)\n",
    "        return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5 QA couples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261f9f19502e46e18da836c721dbeacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_GENERATIONS = 5  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decarbonization levers for scope 3 are complex and performance improvements, rolled out to key that at least half of all aluminum used\\nand often require innovation and partnerships suppliers in the coming years. is composed of secondary aluminum\\nall along the value chain. We embrace this as an by 2030.\\nopportunity to collaborate with our business Downstream emissions:\\nIt was important to us at Danfoss to\\npartners, suppliers, peers, and customers to achieve Decarbonizing with our customers\\nhave a clear path towards delivering on\\nour shared goals of industry decarbonization. Our downstream scope 3 emissions from the use our First Movers Coalition commitment. About the First Movers Coalition\\nof sold products account for around 96% of our total\\nBy the time we announced our ∙ 90+ leading global companies\\nUpstream emissions: carbon footprint and amount to approximately\\ncommitment, we had begun ∙ A partnership by the US State Department and\\nDecarbonizing with our suppliers 122 million tons annually. negotiations with suppliers and had World Economic Forum\\nUpstream emissions make up around 4% of our secured the first contracts for low- ∙ Represents EUR 14 billion in annual demand, the world’s\\ntotal carbon footprint. This is where we have In the segment decarbonization roadmaps, carbon aluminum. largest demand signal\\ninfluence through our partnership with suppliers. we identified levers, opportunities, and also ∙ The seven hard-to-abate sectors covered by the FMC account\\nIn the upstream decarbonization roadmaps roadblocks and learnings. Highlights of levers for 30% of total global emissions\\ndeveloped in 2023, we identified key levers that and opportunities include: further investment\\nwill guide our work in the coming years: in optimization and energy efficiency, transition\\nto next-generation technologies, and automation.</td>\n",
       "      <td>What percentage of Danfoss's total carbon footprint is made up of upstream emissions?\\n</td>\n",
       "      <td>Upstream emissions make up around 4% of Danfoss's total carbon footprint.</td>\n",
       "      <td>ESG Knowledge Library\\Annual-Report-2023.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      context  \\\n",
       "0  The decarbonization levers for scope 3 are complex and performance improvements, rolled out to key that at least half of all aluminum used\\nand often require innovation and partnerships suppliers in the coming years. is composed of secondary aluminum\\nall along the value chain. We embrace this as an by 2030.\\nopportunity to collaborate with our business Downstream emissions:\\nIt was important to us at Danfoss to\\npartners, suppliers, peers, and customers to achieve Decarbonizing with our customers\\nhave a clear path towards delivering on\\nour shared goals of industry decarbonization. Our downstream scope 3 emissions from the use our First Movers Coalition commitment. About the First Movers Coalition\\nof sold products account for around 96% of our total\\nBy the time we announced our ∙ 90+ leading global companies\\nUpstream emissions: carbon footprint and amount to approximately\\ncommitment, we had begun ∙ A partnership by the US State Department and\\nDecarbonizing with our suppliers 122 million tons annually. negotiations with suppliers and had World Economic Forum\\nUpstream emissions make up around 4% of our secured the first contracts for low- ∙ Represents EUR 14 billion in annual demand, the world’s\\ntotal carbon footprint. This is where we have In the segment decarbonization roadmaps, carbon aluminum. largest demand signal\\ninfluence through our partnership with suppliers. we identified levers, opportunities, and also ∙ The seven hard-to-abate sectors covered by the FMC account\\nIn the upstream decarbonization roadmaps roadblocks and learnings. Highlights of levers for 30% of total global emissions\\ndeveloped in 2023, we identified key levers that and opportunities include: further investment\\nwill guide our work in the coming years: in optimization and energy efficiency, transition\\nto next-generation technologies, and automation.   \n",
       "\n",
       "                                                                                  question  \\\n",
       "0  What percentage of Danfoss's total carbon footprint is made up of upstream emissions?\\n   \n",
       "\n",
       "                                                                      answer  \\\n",
       "0  Upstream emissions make up around 4% of Danfoss's total carbon footprint.   \n",
       "\n",
       "                                     source_doc  \n",
       "0  ESG Knowledge Library\\Annual-Report-2023.pdf  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
    "\n",
    "We thus build critique agents that will rate each question on several criteria, given in this paper:\n",
    "\n",
    "Groundedness: can the question be answered from the given context?\n",
    "Relevance: is the question relevant to users? For instance, \"What is the date when transformers 4.29.1 was released?\" is not relevant for ML practicioners.\n",
    "One last failure case we’ve noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like \"What is the name of the function used in this guide?\". We also build a critique agent for this criteria:\n",
    "\n",
    "Stand-alone: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be What is the function used in this article? for a question generated from a specific blog article.\n",
    "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
